{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Variational Autoencoder.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"iXW_JlKExTix","colab_type":"text"},"source":["#Variational Autoencoder with MNIST\n","\n","##Setup\n","\n","VAE with Gaussian Latent Variables and Gaussian Posterior distribution.\n","\n","Specifications:\n","\n","* Latent variable $\\textbf{z} \\sim p (\\textbf{z}) = \\mathcal N_p ({\\bf\\mu}, {\\bf \\Sigma})$, with mean zero and ${\\bf \\Sigma}$ diagonal\n","* Decoder $p(\\mathbf{x}|\\mathbf{z})$ is given by a MLP with a single hidden layer\n","* Encoder $q_\\phi({\\bf z}|{\\bf x})= \\mathcal{N}({\\bf z}, \\boldsymbol{\\mu}({\\bf x}), \\mathrm{diag}(\\boldsymbol{\\sigma}^2({\\bf x})))$\n","* Cost-function consisting of a reconstruction error with a regularization, that minimizes the KL-Divergence. The reconstruction error is the crossentropy between samples and their reconstructions.\n","* KL-Divergence for this setup\n","$$-D_{KL}(q_\\phi({\\bf z}|{\\bf x})|p({\\bf z}))={1 \\over 2} \\sum_{j=1}^J \\left (1+\\log{\\sigma_j^2({\\bf x})}-\\mu_j^2({\\bf x}) -\\sigma_j^2({\\bf x})\\right).\n","$$"]},{"cell_type":"markdown","metadata":{"id":"w4nS1HGG4y3p","colab_type":"text"},"source":["##Import libraries and data"]},{"cell_type":"code","metadata":{"id":"kDNX8lK7xRNb","colab_type":"code","colab":{}},"source":["from variational_autoencoder import VariationalAutoencoder\n","from variational_autoencoder import SampleGenerator\n","from variational_autoencoder import HiddenVariablePlotter\n","from keras.datasets import mnist\n","\n","# Parameters\n","shape_parameters = {'input_dim': 784,\n","              'hidden_dim': 256,\n","              'latent_dim': 2}\n","fit_parameters = {'optimizer': rmsprop, \n","                  'loss': neg_log_likelihood, \n","                  'batch_size': 100,\n","                  'epochs': 3,\n","                  'epsilon_std': 1.0}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PPzHqkmUtoz_","colab_type":"text"},"source":["## Load MNIST Dataset"]},{"cell_type":"code","metadata":{"id":"S9UNAsPKtsbG","colab_type":"code","colab":{}},"source":["# Data\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train = x_train.reshape(-1, input_dim) / 255.\n","x_test = x_test.reshape(-1, input_dim) / 255."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T2TkubWTtvjJ","colab_type":"text"},"source":["## Initialize and train VAE"]},{"cell_type":"code","metadata":{"id":"giAS5I09twGx","colab_type":"code","colab":{}},"source":["# Distributions\n","hidden_prior_dist = 'gaussian'          # p(z):       prior distribution of the hidden variables \n","encoder_approx = 'gaussian_diagonal'    # q_phi(z|x): mean field approximating intractable p(z|x)\n","                                        # (further possible approximations: Mises-Fisher, Gaussian Mixtures)\n","\n","dists = (hidden_prior_dist, encoder_approx)\n","\n","vae = VariationalAutoencoder(*dists, **shape_parameters)\n","\n","vae.fit(x_train, x_test, **fit_parameters)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4cnFAuhxvfhD","colab_type":"text"},"source":["## Plot Samples"]},{"cell_type":"code","metadata":{"id":"kyQVjr2GsQMv","colab_type":"code","colab":{}},"source":["n = 5\n","digit_size = 28\n","plot_size = (10, 10)\n","\n","plot = HiddenVariablePlotter(n, digit_size, plot_size)\n","plot.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SnTDIffqxPft","colab_type":"text"},"source":["## Plot embedding in hidden variable space"]},{"cell_type":"code","metadata":{"id":"MJ0md7lYxX-f","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}